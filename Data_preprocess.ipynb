{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------- Data Preprocessing -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str_sst(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for the SST dataset\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)   \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase -> index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232 14058\n"
     ]
    }
   ],
   "source": [
    "phr_to_ind = dict()\n",
    "\n",
    "with open('./Datasets/SST1_dataset/dictionary.txt') as f:\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        phr_to_ind[entry[0]] = int(entry[1])\n",
    "\n",
    "keys = phr_to_ind.keys();\n",
    "\n",
    "print(len(phr_to_ind), phr_to_ind['Good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Index corresponding to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n"
     ]
    }
   ],
   "source": [
    "# Without doing the below computation directly load the stored output\n",
    "sentence_list = []\n",
    "sentiment = []\n",
    "\n",
    "with open('./Datasets/SST1_dataset/SentenceWithCorrection.txt') as f:\n",
    "    for line in f:\n",
    "        sent = line[:-1]\n",
    "#         if len(sent.split()) < 3:\n",
    "#             continue\n",
    "            \n",
    "        sentiment.append(phr_to_ind[sent])\n",
    "        sentence_list.append(clean_str_sst(sent))\n",
    "\n",
    "print(len(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence_list = []\n",
    "# sentiment = []\n",
    "\n",
    "# with open('../../Datasets/SST1_dataset/datasetSentences.txt') as f:\n",
    "#     f.readline()\n",
    "#     for line in f:\n",
    "#         entry = line.split('\\t')\n",
    "#         sent = entry[1][:-1]\n",
    "#         sent = sent.replace('-LRB-', '(')\n",
    "#         sent = sent.replace('-RRB-', ')')\n",
    "    \n",
    "#         if sent in phr_to_ind.keys():\n",
    "#             sentiment.append(phr_to_ind[sent])\n",
    "#         else:\n",
    "#             print('.', end=\"\")\n",
    "#             keys_subset = [k for k in keys if (k[0] == sent[0])]\n",
    "#             key = difflib.get_close_matches(sent, keys_subset, n=1);\n",
    "#             sent = key[0]\n",
    "#             sentiment.append(phr_to_ind[sent])\n",
    "            \n",
    "#         sentence_list.append(sent)\n",
    "        \n",
    "# print(len(sentence_list))\n",
    "\n",
    "# # Written the output in a file\n",
    "# f = open('../../Datasets/SST1_dataset/SentenceWithCorrection.txt', 'w')\n",
    "# for sent in sentence_list:\n",
    "#     f.write(sent + '\\n')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Index -> Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232\n"
     ]
    }
   ],
   "source": [
    "ind_to_senti = dict()\n",
    "\n",
    "with open('./Datasets/SST1_dataset/sentiment_labels.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        ind_to_senti[int(entry[0])] = float(entry[1])\n",
    "\n",
    "print(len(ind_to_senti))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading train, test and valid split info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n",
      "9645 2210 0\n"
     ]
    }
   ],
   "source": [
    "split_ind = []\n",
    "with open('./Datasets/SST1_dataset/datasetSplit.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split(',')\n",
    "        split_ind.append(int(entry[1]))\n",
    "\n",
    "print(len(split_ind))\n",
    "\n",
    "for i in range(len(split_ind)):\n",
    "    if split_ind[i] == 3:\n",
    "        split_ind[i] = 1\n",
    "        \n",
    "N_train = split_ind.count(1)\n",
    "N_test = split_ind.count(2)\n",
    "N_valid = split_ind.count(3)\n",
    "print (N_train, N_test, N_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning label to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510 3140 2242 3111\n"
     ]
    }
   ],
   "source": [
    "N_sent = len(sentence_list);\n",
    "N_category = 5\n",
    "\n",
    "y_label = []\n",
    "\n",
    "for ind in sentiment:\n",
    "    val = ind_to_senti[ind]\n",
    "    if val >= 0.0 and val <= 0.2:\n",
    "        y_label.append(0);\n",
    "    elif val > 0.2 and val <= 0.4:\n",
    "        y_label.append(1)\n",
    "    elif val > 0.4 and val <= 0.6:\n",
    "        y_label.append(2)\n",
    "    elif val > 0.6 and val <= 0.8:\n",
    "        y_label.append(3)\n",
    "    else:\n",
    "        y_label.append(4)\n",
    "\n",
    "print(y_label.count(0), y_label.count(1), y_label.count(2), y_label.count(3))\n",
    "\n",
    "# Labels in one-hot encoding\n",
    "y_train = np.zeros((N_train, N_category), np.uint8)\n",
    "y_test  = np.zeros((N_test , N_category), np.uint8)\n",
    "y_valid = np.zeros((N_valid, N_category), np.uint8)\n",
    "\n",
    "c1,c2,c3 = 0,0,0\n",
    "for i in range(len(y_label)):\n",
    "    label = y_label[i]\n",
    "    if split_ind[i] == 1:\n",
    "        y_train[c1, label] = 1;  c1 += 1\n",
    "    elif split_ind[i] == 2:\n",
    "        y_test [c2, label] = 1;  c2 += 1\n",
    "    else:\n",
    "        y_valid[c3, label] = 1;  c3 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232 17836\n"
     ]
    }
   ],
   "source": [
    "x_all = []\n",
    "max_sent_len = -1;\n",
    "max_wrd_len = -1\n",
    "wrd_to_ind = dict()\n",
    "\n",
    "ind_new = 1;\n",
    "for sent in sentence_list:\n",
    "    wrds = sent.split()\n",
    "    vec = []\n",
    "    for wrd in wrds:\n",
    "        if wrd not in wrd_to_ind.keys():\n",
    "            wrd_to_ind[wrd] = ind_new\n",
    "            ind_new += 1\n",
    "            \n",
    "        ind = wrd_to_ind[wrd]\n",
    "        vec.append(ind)\n",
    "            \n",
    "    max_sent_len = max(len(vec), max_sent_len)\n",
    "    x_all.append(vec)\n",
    "\n",
    "# Get inverse dictionary\n",
    "ind_to_wrd = dict((v, k) for k, v in wrd_to_ind.items())\n",
    "ind_to_wrd[0] = \"<PAD/>\"\n",
    "\n",
    "print(len(phr_to_ind), len(wrd_to_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9645 2210 0\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "x_valid = []\n",
    "\n",
    "c1, c2, c3 = 0,0,0\n",
    "for i in range(len(x_all)):\n",
    "    vec = x_all[i]\n",
    "    if split_ind[i] == 1:\n",
    "        x_train.append(vec)\n",
    "        c1 += 1\n",
    "    elif split_ind[i] == 2:\n",
    "        x_test.append(vec)\n",
    "        c2 += 1\n",
    "    else:\n",
    "        x_valid.append(vec)\n",
    "        c3 += 1\n",
    "\n",
    "print(c1, c2, c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Phrases':sentence_list, 'Label':y_label, 'split_ind':split_ind})\n",
    "raw_docs_train      = df[df.split_ind == 1]['Phrases'].values\n",
    "sentiment_train     = df[df.split_ind == 1]['Label'].values\n",
    "raw_docs_test       = df[df.split_ind == 2]['Phrases'].values\n",
    "sentiment_test      = df[df.split_ind == 2]['Label'].values\n",
    "num_labels          = len(np.unique(sentiment_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train docs...\n",
      "pre-processing test docs...\n",
      "converting to token ids...\n",
      "(9645, 37) (2210, 37) (9645, 5) (2210, 5)\n"
     ]
    }
   ],
   "source": [
    "#text pre-processing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print (\"pre-processing train docs...\")\n",
    "processed_docs_train = []\n",
    "for doc in raw_docs_train:\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = tokens\n",
    "#     filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    processed_docs_train.append(stemmed)\n",
    "\n",
    "print (\"pre-processing test docs...\")\n",
    "processed_docs_test = []\n",
    "for doc in raw_docs_test:\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = tokens\n",
    "#    filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    processed_docs_test.append(stemmed)\n",
    "    \n",
    "processed_docs_all = np.concatenate((processed_docs_train, processed_docs_test), axis=0)\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_docs_all)\n",
    "dictionary_size = len(dictionary.keys())\n",
    "\n",
    "print (\"converting to token ids...\")\n",
    "word_id_train, word_id_len = [], []\n",
    "for doc in processed_docs_train:\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_train.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))\n",
    "\n",
    "word_id_test, word_ids = [], []\n",
    "for doc in processed_docs_test:\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_test.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))\n",
    "        \n",
    "seq_len = np.round((np.mean(word_id_len) + 2*np.std(word_id_len))).astype(int)\n",
    "\n",
    "#pad sequences\n",
    "x_train = sequence.pad_sequences(np.array(word_id_train), maxlen=seq_len, padding='post', value=dictionary_size)\n",
    "x_test  = sequence.pad_sequences(np.array(word_id_test), maxlen=seq_len, padding='post', value=dictionary_size)\n",
    "y_train = np_utils.to_categorical(sentiment_train, num_labels)\n",
    "y_test  = np_utils.to_categorical(sentiment_test, num_labels)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_inv = dict((v, k) for k, v in dictionary.token2id.items())\n",
    "vocabulary_inv[dictionary_size] = \"<PAD/>\"\n",
    "dictionary.id2token = vocabulary_inv\n",
    "dictionary.seq_len = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open( './Datasets/SST1_dataset/sst_data', 'wb') as data:\n",
    "    pickle.dump([x_train, y_train, x_test, y_test, dictionary], data, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
